{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV4exPjjioqr"
      },
      "outputs": [],
      "source": [
        "# Dependencies\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from transformers import pipeline\n",
        "from scipy.stats import entropy\n",
        "import torch\n",
        "from gnews import GNews\n",
        "import yfinance as yf\n",
        "from datetime import date,datetime, timedelta\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq3zArCnkjcs"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ompGi_7dR1tg"
      },
      "source": [
        "##### Data Loading of Stock tweets data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r63aie2NRwSA"
      },
      "outputs": [],
      "source": [
        "file_path0=\"/content/stock_tweets.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tmpvh3T1SCeG",
        "outputId": "6ebc5b8a-90b6-4e3c-935a-3de5f9242653"
      },
      "outputs": [],
      "source": [
        "df0 = pd.read_csv(file_path0)\n",
        "df0.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHKO4W16R-6p",
        "outputId": "3481ecdd-a2c4-4756-fbe8-915c2cf4b7a6"
      },
      "outputs": [],
      "source": [
        "df0.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNbpiA3QTrBF",
        "outputId": "6aae009f-88e9-4fab-8ed2-91d6424b0313"
      },
      "outputs": [],
      "source": [
        "df0.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "oZRYEDUfTiAD",
        "outputId": "135d3d06-f6c1-4032-ab27-4b6b30ff68ff"
      },
      "outputs": [],
      "source": [
        "df0.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkc9LQq6TmBZ",
        "outputId": "fdb31610-9417-47e5-c5be-0b25b9a333f2"
      },
      "outputs": [],
      "source": [
        "df0.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmkB6WSvSN4v",
        "outputId": "a5ea0dbc-b885-4d71-a5aa-3b9117228725"
      },
      "outputs": [],
      "source": [
        "# Ascending order (earliest first)\n",
        "print(\"Five earliest dates:\")\n",
        "print(df0['Date'].sort_values().head(5))\n",
        "\n",
        "# Descending order (latest first)\n",
        "print(\"\\nFive latest dates:\")\n",
        "print(df0['Date'].sort_values(ascending=False).head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kgv_AmjSUsa"
      },
      "source": [
        "###### Data is from september 2021 to september 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF6pkeGby2A3"
      },
      "source": [
        "##### Data Loading of Stock OHCLV data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pFzUE_9ivfO"
      },
      "outputs": [],
      "source": [
        "file_path=\"/content/stock_yfinance_data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ExiY0aT2j4AQ",
        "outputId": "613dd5d2-188c-4681-b470-5c95f7f71d14"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9iERjdskfI7",
        "outputId": "f291760b-eb15-4a56-f05b-93dc81558fbe"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JSCpyF6nALY",
        "outputId": "058b2cc4-3eb3-42e4-a892-2615b207520c"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "kkhEN8is9REL",
        "outputId": "1a33aeed-b496-49bd-f417-3e61cc46ff6a"
      },
      "outputs": [],
      "source": [
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnozpMpnkpma",
        "outputId": "e6167132-74c7-4233-9635-2b6685090f16"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnGkdGkWtB2Z",
        "outputId": "8e425552-556e-4d49-e349-c7019960d5ff"
      },
      "outputs": [],
      "source": [
        "print(\"Data index\")\n",
        "print(\"\\nPrint ascending order dates\")\n",
        "print(df['Date'].sort_values().head(5))\n",
        "print(\"\\nPrint descending order dates\")\n",
        "print(df['Date'].sort_values(ascending=False).head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJjWxrK2Auzd"
      },
      "source": [
        "##### Dataset is from *Sept*-2021 to Sept-2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OskT8OkDkwPk"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbOhHGNrUabE"
      },
      "source": [
        "###### EDA on Stock Tweets data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k25bAX8lRWl"
      },
      "outputs": [],
      "source": [
        "# Convert date column into standardized form\n",
        "df0['Date'] = pd.to_datetime(df0['Date'], utc=True).dt.tz_localize(None).dt.normalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfVWJJ43ULcl"
      },
      "outputs": [],
      "source": [
        "df0['Company Name'] = df0['Company Name'].str.replace(' ', '')\n",
        "df0['Stock Name'] = df0['Stock Name'].astype('string').str.upper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBA2Q69lUNJH",
        "outputId": "270599c3-dc39-4be4-fdb5-0293c4cc082a"
      },
      "outputs": [],
      "source": [
        "df0.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy8vGerFUmj7",
        "outputId": "739535f5-fc8a-4268-a76b-8b6ba319b5ed"
      },
      "outputs": [],
      "source": [
        "# Missing\n",
        "print(df0.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90NyCBpbVkbG",
        "outputId": "3c6bfc97-0983-46a0-a11f-53e74e3e4681"
      },
      "outputs": [],
      "source": [
        "# Cardinality\n",
        "print(\"Unique dates:\", df0['Date'].nunique())\n",
        "print(\"Unique tickers:\", df0['Stock Name'].nunique())\n",
        "print(df0['Stock Name'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "QrWa6v6jUrU1",
        "outputId": "0bbe9eaf-3fed-4dd8-a739-eb315584090d"
      },
      "outputs": [],
      "source": [
        "# Count tweets per ticker\n",
        "ticker_counts = df0['Stock Name'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "# Line plot\n",
        "plt.plot(ticker_counts.index, ticker_counts.values, marker='o')\n",
        "\n",
        "plt.title(\"Tweet Count per Ticker (Line Plot)\")\n",
        "plt.xlabel(\"Stock Name\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "yWxaG5TsWEC1",
        "outputId": "97a5b775-23c4-4cb3-f739-421e662b9d8c"
      },
      "outputs": [],
      "source": [
        "# Class Imbalance\n",
        "before_counts = df0['Stock Name'].value_counts()\n",
        "min_count = before_counts.min()\n",
        "\n",
        "df_balanced = (\n",
        "    df0.groupby('Stock Name', group_keys=False)\n",
        "       .apply(lambda x: x.sample(min_count, random_state=42))\n",
        ")\n",
        "balanced_counts = df_balanced['Stock Name'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(balanced_counts.index, balanced_counts.values)\n",
        "plt.title(\"Class Distribution After Balancing (Downsampling)\")\n",
        "plt.xlabel(\"Stock Name\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "ZKs2ugk0Vx7u",
        "outputId": "c0a30cc4-ff72-423b-ad2c-0c2ea83499cd"
      },
      "outputs": [],
      "source": [
        "df0['tweet_len'] = df0['Tweet'].str.len()\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.histplot(df0['tweet_len'], bins=50, kde=True)\n",
        "plt.title(\"Tweet length distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "O5KOzmSzV8mq",
        "outputId": "f0af99d2-9ed5-435b-aa14-d374fe85958e"
      },
      "outputs": [],
      "source": [
        "tweets_per_day = df0.groupby('Date').size()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "tweets_per_day.plot()\n",
        "plt.title(\"Tweets per day\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5QAEpOBW3BB"
      },
      "source": [
        "##### EDA on Stock OHCLV data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj-vNIp_lRWm"
      },
      "outputs": [],
      "source": [
        "# Convert date column into standardized form\n",
        "df['Date'] = pd.to_datetime(df['Date'], utc=True).dt.tz_localize(None).dt.normalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91UB0oVKlRWm"
      },
      "outputs": [],
      "source": [
        "#Convert Variables into standard format\n",
        "df['Stock Name'] = df['Stock Name'].astype('string').str.upper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "Y5tM2SI0ksKz",
        "outputId": "990644ae-f283-4d43-e2d9-9d6709b02fb5"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycKdfNQtk1Ej",
        "outputId": "9f5a4abe-a961-4003-fea3-968df2430b14"
      },
      "outputs": [],
      "source": [
        "df['has_space'] = df['Stock Name'].str.contains(' ')\n",
        "print(df[df['has_space'] == 'True'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "ja1BMWCXoWn-",
        "outputId": "0655d475-a7a2-4136-f236-6717e89bb3a9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(3,2,1)\n",
        "sns.histplot(df['Open'],kde=True)\n",
        "plt.xlabel('Open')\n",
        "plt.subplot(3,2,2)\n",
        "sns.histplot(df['High'],kde=True)\n",
        "plt.xlabel('High')\n",
        "plt.subplot(3,2,3)\n",
        "sns.histplot(df['Low'],kde=True)\n",
        "plt.xlabel('Low')\n",
        "plt.subplot(3,2,4)\n",
        "sns.histplot(df['Close'],kde=True)\n",
        "plt.xlabel('Close')\n",
        "plt.subplot(3,2,5)\n",
        "sns.histplot(df['Adj Close'],kde=True)\n",
        "plt.xlabel('Adj Close')\n",
        "plt.subplot(3,2,6)\n",
        "sns.histplot(df['Volume'],kde=True)\n",
        "plt.xlabel('Volume')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbrFdpWbpZm3",
        "outputId": "5dde5270-0d68-4421-fa57-f198cc97786d"
      },
      "outputs": [],
      "source": [
        "df_outliers = df.copy()\n",
        "numerical_columns = df_outliers.select_dtypes(include=['int64','float64'])\n",
        "for col in numerical_columns:\n",
        "  Q1 = df_outliers[col].quantile(0.25)\n",
        "  Q3 = df_outliers[col].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  upper_bound = Q3 + 1.5 * IQR\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  df_outliers = df_outliers[(df_outliers[col] > lower_bound) & (df_outliers[col] <= upper_bound)]\n",
        "print(f\"Number of outliers removed:{len(df) - len(df_outliers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "GR6qBxJm1BVV",
        "outputId": "e6da228c-ca1e-44cc-d65b-58064a419b1d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(3,2,1)\n",
        "sns.histplot(df_outliers['Open'],kde=True)\n",
        "plt.xlabel('Open')\n",
        "plt.subplot(3,2,2)\n",
        "sns.histplot(df_outliers['High'],kde=True)\n",
        "plt.xlabel('High')\n",
        "plt.subplot(3,2,3)\n",
        "sns.histplot(df_outliers['Low'],kde=True)\n",
        "plt.xlabel('Low')\n",
        "plt.subplot(3,2,4)\n",
        "sns.histplot(df_outliers['Close'],kde=True)\n",
        "plt.xlabel('Close')\n",
        "plt.subplot(3,2,5)\n",
        "sns.histplot(df_outliers['Adj Close'],kde=True)\n",
        "plt.xlabel('Adj Close')\n",
        "plt.subplot(3,2,6)\n",
        "sns.histplot(df_outliers['Volume'],kde=True)\n",
        "plt.xlabel('Volume')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3-5m3qFo8Xh"
      },
      "outputs": [],
      "source": [
        "counts = df_outliers['Stock Name'].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4pZ0Rqyn18wS",
        "outputId": "692e4a74-ffa2-47b5-98af-00080734899c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,12))\n",
        "sns.barplot(y=counts.index, x=counts.values,color='skyblue',edgecolor='black')\n",
        "plt.xlabel('Stock Name')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Stock Name Distribution')\n",
        "plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "CAzh5tiI2Zcf",
        "outputId": "ce9161b4-87e4-4b22-d79f-edd06f73ad13"
      },
      "outputs": [],
      "source": [
        "corr_matrix = numerical_columns.corr()\n",
        "print(\"Correlation matrix\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of OHCLV Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsTfsd37BCdB"
      },
      "source": [
        "#### We see that the columns are highy corelated but we are not going to remove the multicolinearity issue as we expect columns to be highly colinear for stock prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHfFgAmtx-Yh",
        "outputId": "4fc58e44-ce81-40c7-b3f0-1076521d73d0"
      },
      "outputs": [],
      "source": [
        "inertia_scores = []\n",
        "silhouette_scores = []\n",
        "n_clusters = range(2,11)\n",
        "\n",
        "for n_cluster in n_clusters:\n",
        "  kmeans = KMeans(n_clusters= n_cluster,random_state=42)\n",
        "  kmeans.fit(df[['Adj Close']])\n",
        "  inertia_scores.append(kmeans.inertia_)\n",
        "  silhouette_scores.append(silhouette_score(df[['Adj Close']],kmeans.labels_))\n",
        "print(f\"Inertia Scores:{inertia_scores}\")\n",
        "print(f\"Silhouette Scores:{silhouette_scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "-YRtrUeO_mNY",
        "outputId": "72211b83-9c95-463f-c650-638a77692dc4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(n_clusters,silhouette_scores,marker='o',color='blue')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Scores')\n",
        "plt.title('Silhouette Scores vs Number of Clusters')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(n_clusters,inertia_scores,marker='o',color='blue')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia Scores')\n",
        "plt.title('Inertia Scores vs Number of Clusters')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_VYOjgClRWn"
      },
      "outputs": [],
      "source": [
        "# Need to think how we ca implement clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuQ5yJcflRWn",
        "outputId": "9f6fffc5-3c67-44dc-ef11-4d5c12d06d08"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "print(\"=\" * 70)\n",
        "print(\"EMOTION CLASSIFICATION - DEVICE DETECTION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Detect device\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "if device == 0:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU DETECTED: {gpu_name}\")\n",
        "    print(f\"  Memory: {gpu_memory:.2f} GB\")\n",
        "    device_type = f\"GPU: {gpu_name}\"\n",
        "    # Enable GPU memory optimization\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\"GPU not available, using CPU\")\n",
        "    device_type = \"CPU\"\n",
        "\n",
        "print(f\"Processing device: {device_type}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nLoading emotion classification models...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Load Emtract model (DistilBERT - faster)\n",
        "print(\"Loading Emtract (DistilBERT) model...\")\n",
        "emtract_classifier = pipeline(\n",
        "    'text-classification',\n",
        "    model='vamossyd/emtract-distilbert-base-uncased-emotion',\n",
        "    top_k=None,\n",
        "    device=device,\n",
        "    model_kwargs={'torch_dtype': torch.float16} if device == 0 else {}\n",
        ")\n",
        "print(f\"Emtract model loaded on {device_type}\")\n",
        "\n",
        "# Load Hartmann model (RoBERTa - more detailed)\n",
        "print(\"Loading Hartmann (RoBERTa-large) model...\")\n",
        "hartmann_classifier = pipeline(\n",
        "    'text-classification',\n",
        "    model='j-hartmann/emotion-english-roberta-large',\n",
        "    top_k=None,\n",
        "    device=device,\n",
        "    model_kwargs={'torch_dtype': torch.float16} if device == 0 else {}\n",
        ")\n",
        "print(f\"Hartmann model loaded on {device_type}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "def get_all_scores(tweet, classifier, model_name):\n",
        "    if pd.isna(tweet) or not isinstance(tweet, str):\n",
        "        return {f'{model_name}_Emotion': 'UNKNOWN'}\n",
        "\n",
        "    try:\n",
        "        results = classifier(tweet)\n",
        "        if isinstance(results, list):\n",
        "            results = results[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying tweet: {e}\")\n",
        "        return {f'{model_name}_Emotion': 'ERROR'}\n",
        "\n",
        "    output = {}\n",
        "    output[f'{model_name}_Emotion'] = results[0]['label']\n",
        "\n",
        "    for item in results:\n",
        "        output[f'{model_name}_{item[\"label\"]}_Score'] = item['score']\n",
        "\n",
        "    return output\n",
        "\n",
        "def batch_classify(tweets, classifier, model_name, batch_size=96):\n",
        "    results_list = []\n",
        "    tweets_list = tweets.tolist()\n",
        "\n",
        "    # Filter out NaN and non-string values\n",
        "    valid_tweets = []\n",
        "    valid_indices = []\n",
        "    for idx, tweet in enumerate(tweets_list):\n",
        "        if pd.notna(tweet) and isinstance(tweet, str):\n",
        "            valid_tweets.append(tweet)\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    truncation_warnings = 0\n",
        "\n",
        "    # Process in batches with optimizations\n",
        "    for i in tqdm(range(0, len(valid_tweets), batch_size), desc=f\"Processing {model_name}\"):\n",
        "        batch = valid_tweets[i:i+batch_size]\n",
        "        try:\n",
        "            # Use no_grad context for inference to save memory\n",
        "            with torch.no_grad():\n",
        "                # Process FULL tweets without truncation for better accuracy\n",
        "                batch_results = classifier(batch, top_k=None)\n",
        "\n",
        "            for result_idx, results in enumerate(batch_results):\n",
        "                if not isinstance(results, list):\n",
        "                    results = [results]\n",
        "\n",
        "                output = {}\n",
        "                output[f'{model_name}_Emotion'] = results[0]['label']\n",
        "\n",
        "                for item in results:\n",
        "                    output[f'{model_name}_{item[\"label\"]}_Score'] = item['score']\n",
        "\n",
        "                results_list.append((valid_indices[i + result_idx], output))\n",
        "        except RuntimeError as e:\n",
        "            # Handle token length errors gracefully\n",
        "            if \"token\" in str(e).lower() or \"length\" in str(e).lower():\n",
        "                truncation_warnings += len(batch)\n",
        "                # Fallback: process with truncation for very long tweets\n",
        "                for tweet_idx, tweet_text in enumerate(batch):\n",
        "                    try:\n",
        "                        with torch.no_grad():\n",
        "                            result = classifier(tweet_text, truncation=True, top_k=None)\n",
        "                            if isinstance(result, list):\n",
        "                                result = result[0]\n",
        "                        results_list.append((valid_indices[i + tweet_idx], {\n",
        "                            f'{model_name}_Emotion': result[0]['label'],\n",
        "                            **{f'{model_name}_{item[\"label\"]}_Score': item['score'] for item in result}\n",
        "                        }))\n",
        "                    except Exception as e2:\n",
        "                        results_list.append((valid_indices[i + tweet_idx], {f'{model_name}_Emotion': 'ERROR'}))\n",
        "            else:\n",
        "                print(f\"Error processing batch: {e}\")\n",
        "                for result_idx in range(len(batch)):\n",
        "                    results_list.append((valid_indices[i + result_idx], {f'{model_name}_Emotion': 'ERROR'}))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch: {e}\")\n",
        "            for result_idx in range(len(batch)):\n",
        "                results_list.append((valid_indices[i + result_idx], {f'{model_name}_Emotion': 'ERROR'}))\n",
        "\n",
        "        # Periodic GPU cache cleanup (only clear GPU, not system memory with gc.collect)\n",
        "        if device == 0 and (i // batch_size) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    if truncation_warnings > 0:\n",
        "        print(f\"{truncation_warnings} tweets exceeded max token length (fallback to truncation applied)\")\n",
        "\n",
        "    # Create DataFrame with proper indexing\n",
        "    results_df = pd.DataFrame([output for _, output in results_list])\n",
        "    results_df.index = [idx for idx, _ in results_list]\n",
        "    return results_df.reindex(range(len(tweets_list))).fillna({f'{model_name}_Emotion': 'UNKNOWN'})\n",
        "\n",
        "# Apply classifications\n",
        "print(f\"\\nApplying emotion classification ({device_type})...\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Processing FULL tweets WITHOUT truncation for maximum accuracy\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(f\"[1/2] Classifying with Emtract model (batch_size=96, full tweet processing)...\")\n",
        "start_time = time.time()\n",
        "df0_all_emotions = df0.copy()\n",
        "emtract_results = batch_classify(df0_all_emotions['Tweet'], emtract_classifier, 'Emtract', batch_size=96)\n",
        "df0_all_emotions = pd.concat([df0_all_emotions, emtract_results], axis=1)\n",
        "emtract_time = time.time() - start_time\n",
        "print(f\"Emtract completed in {emtract_time:.2f} seconds ({len(df0_all_emotions)} tweets)\")\n",
        "print(f\"Emtract results saved in DataFrame (columns: {len(emtract_results.columns)})\")\n",
        "\n",
        "# Delete intermediate variable to free memory, but data is safe in df0\n",
        "del emtract_results\n",
        "if device == 0:\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"[2/2] Classifying with Hartmann model (batch_size=96, full tweet processing)...\")\n",
        "start_time = time.time()\n",
        "hartmann_results = batch_classify(df0_all_emotions['Tweet'], hartmann_classifier, 'Hartmann', batch_size=96)\n",
        "df0_all_emotions = pd.concat([df0_all_emotions, hartmann_results], axis=1)\n",
        "hartmann_time = time.time() - start_time\n",
        "print(f\"Hartmann completed in {hartmann_time:.2f} seconds ({len(df0_all_emotions)} tweets)\")\n",
        "print(f\"Hartmann results saved in DataFrame (columns: {len(hartmann_results.columns)})\")\n",
        "\n",
        "# Delete intermediate variable to free memory, but data is safe in df0\n",
        "del hartmann_results\n",
        "\n",
        "# Save results IMMEDIATELY after processing (data is now in df0_all_emotions)\n",
        "print(\"\\nSaving combined results to file...\")\n",
        "df0_all_emotions.to_parquet('stock_tweets_with_all_scores.parquet', index=False)\n",
        "print(f\"✓ Results saved: {df0_all_emotions.shape[0]} rows × {df0_all_emotions.shape[1]} columns\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"\\n CLASSIFICATION COMPLETE\")\n",
        "print(f\"  Device used: {device_type}\")\n",
        "print(f\"  Total processing time: {emtract_time + hartmann_time:.2f} seconds\")\n",
        "print(f\"  Tweets processed: {len(df0_all_emotions)}\")\n",
        "print(f\"  Avg time per tweet: {((emtract_time + hartmann_time) / len(df0_all_emotions)):.4f}s\")\n",
        "print(f\"  Processing mode: FULL tweets \")\n",
        "print(f\"  Output file: stock_tweets_with_all_scores.parquet\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Final GPU cleanup (AFTER saving)\n",
        "if device == 0:\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUZJv_YNX4Kc"
      },
      "source": [
        "###### Load the sentiments in the stock tweets data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFzETA40lqfF"
      },
      "source": [
        "###### We have loaded the dataset with both hartmann and emtract model scores and compare the values with the help of two metrics: Shanon Entropy and Valence Bias which falls under fidelity test to validate the performance of our emtract model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOf8Cebe59QS",
        "outputId": "ff9bdbea-1ab8-4413-fc47-affd18e95b14"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_parquet('/content/stock_tweets_with_all_scores.parquet')\n",
        "def compute_entropy(df, prefix):\n",
        "    prob_cols = [c for c in df.columns if c.startswith(prefix) and c.endswith('_Score')]\n",
        "    prob_cols = sorted(prob_cols)\n",
        "    probs = df[prob_cols].values\n",
        "    return np.apply_along_axis(lambda x: entropy(x, base=2), 1, probs), prob_cols\n",
        "\n",
        "# Emtract entropy\n",
        "df2['Emtract_Entropy'], emtract_cols = compute_entropy(df2, 'Emtract_')\n",
        "\n",
        "# Hartmann entropy\n",
        "df2['Hartmann_Entropy'], hartmann_cols = compute_entropy(df2, 'Hartmann_')\n",
        "\n",
        "# Final Comparison in entropy of the models\n",
        "avg_emtract_h = df2['Emtract_Entropy'].mean()\n",
        "avg_hartmann_h = df2['Hartmann_Entropy'].mean()\n",
        "print(f\"Average entropy of emtract model:{avg_emtract_h:.2f}\")\n",
        "print(f\"Average entropy of hartmann model:{avg_hartmann_h:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRqjOx6YmPwe"
      },
      "source": [
        "##### We can observe that the entropy of hartmann model is greater than entropy model which concludes that the hartmann model is less infrmative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXYWfX4PaSnl",
        "outputId": "f7964ab8-7ff3-46fe-bc24-d5a00d22f121"
      },
      "outputs": [],
      "source": [
        "strong_signal_threshold = 0.5\n",
        "EMOTION_ALIASES = {\n",
        "    \"Happy\": [\"happy\", \"happiness\", \"joy\"],\n",
        "    \"Sad\": [\"sad\", \"sadness\", \"unhappy\"],\n",
        "    \"Anger\": [\"anger\", \"angry\", \"rage\"],\n",
        "    \"Fear\": [\"fear\", \"afraid\", \"anxiety\"],\n",
        "    \"Disgust\": [\"disgust\", \"disgusted\"]\n",
        "}\n",
        "def find_emotion_columns(df2, prefix, emotions):\n",
        "    cols = []\n",
        "    for col in df2.columns:\n",
        "        if not col.startswith(prefix):\n",
        "            continue\n",
        "        if not col.endswith(\"_Score\"):\n",
        "            continue\n",
        "\n",
        "        col_l = col.lower()\n",
        "        for emo in emotions:\n",
        "            for alias in EMOTION_ALIASES[emo]:\n",
        "                if alias in col_l:\n",
        "                    cols.append(col)\n",
        "                    break\n",
        "    return list(set(cols))\n",
        "\n",
        "\n",
        "def compute_valence(df2, prefix):\n",
        "    # Positive emotions\n",
        "    pos_cols = find_emotion_columns(df2, prefix, [\"Happy\"])\n",
        "\n",
        "    # Negative emotions\n",
        "    neg_cols = find_emotion_columns(\n",
        "        df2, prefix, [\"Sad\", \"Anger\", \"Fear\", \"Disgust\"]\n",
        "    )\n",
        "\n",
        "    if len(pos_cols) == 0 or len(neg_cols) == 0:\n",
        "        raise ValueError(f\"Emotion columns missing for {prefix}\")\n",
        "\n",
        "    pos_score = df2[pos_cols].sum(axis=1)\n",
        "    neg_score = df2[neg_cols].sum(axis=1)\n",
        "\n",
        "    return pos_score - neg_score, pos_cols, neg_cols\n",
        "print(\"Calculating Valence Scores\")\n",
        "\n",
        "df2['Emtract_Valence'], em_pos, em_neg = compute_valence(df2, \"Emtract\")\n",
        "df2['Hartmann_Valence'], ha_pos, ha_neg = compute_valence(df2, \"Hartmann\")\n",
        "# Signal intensity (mean absolute valence)\n",
        "emtract_intensity = df2['Emtract_Valence'].abs().mean()\n",
        "hartmann_intensity = df2['Hartmann_Valence'].abs().mean()\n",
        "# Actionable rate\n",
        "emtract_actionable = (\n",
        "    df2['Emtract_Valence'].abs() > strong_signal_threshold\n",
        ").mean() * 100\n",
        "\n",
        "hartmann_actionable = (\n",
        "    df2['Hartmann_Valence'].abs() > strong_signal_threshold\n",
        ").mean() * 100\n",
        "# Contradiction rate\n",
        "contradictions = df2[\n",
        "    ((df2['Emtract_Valence'] > 0.1) & (df2['Hartmann_Valence'] < -0.1)) |\n",
        "    ((df2['Emtract_Valence'] < -0.1) & (df2['Hartmann_Valence'] > 0.1))\n",
        "]\n",
        "contradiction_rate = (len(contradictions) / len(df2)) * 100\n",
        "\n",
        "# Print results\n",
        "print(\"\\n MODEL UTILITY COMPARISON\")\n",
        "\n",
        "print(\"\\n SIGNAL INTENSITY (Avg Absolute Valence Score: 0–1)\")\n",
        "print(\"(Higher = More decisive emotions)\")\n",
        "print(f\"Emtract:  {emtract_intensity:.4f}\")\n",
        "print(f\"Hartmann: {hartmann_intensity:.4f}\")\n",
        "\n",
        "print(f\"\\n ACTIONABLE RATE (|Valence| > {strong_signal_threshold})\")\n",
        "print(\"(Higher = More potential trading triggers)\")\n",
        "print(f\"Emtract:  {emtract_actionable:.2f}%\")\n",
        "print(f\"Hartmann: {hartmann_actionable:.2f}%\")\n",
        "\n",
        "print(\"\\n CONTRADICTION RATE\")\n",
        "print(\"(How often do they completely disagree?)\")\n",
        "print(f\"{contradiction_rate:.2f}% of tweets have opposite sentiments.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKSwpwpYczZs",
        "outputId": "be13b9b5-f706-4fcc-81d7-e7957cbaa6cd"
      },
      "outputs": [],
      "source": [
        "# Calculating the average valence of the model\n",
        "avg_emtract_valence = df2['Emtract_Valence'].mean()\n",
        "avg_hartmann_valence = df2['Hartmann_Valence'].mean()\n",
        "\n",
        "print(f\"Average Emtract Valence (Bias): {avg_emtract_valence:.4f}\")\n",
        "print(f\"Average Hartmann Valence (Bias): {avg_hartmann_valence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki19G-nsmvRN"
      },
      "source": [
        "###### We can observe that the hartmann model is more negatively biased than our emtract model, concluding the fact that emtract model is more suited for stock prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzckcK-giMb",
        "outputId": "683dcb3f-3626-475b-ccae-9ca85fd6c5e3"
      },
      "outputs": [],
      "source": [
        "df0_model = df0.copy()\n",
        "\n",
        "# Loading the emtract model for consumption in data pipeline\n",
        "print(\"=\" * 70)\n",
        "print(\"EMOTION CLASSIFICATION - SIMPLIFIED PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "if device == 0:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\" GPU DETECTED: {gpu_name}\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\" Using CPU\")\n",
        "\n",
        "emotion_classifier = pipeline('text-classification',\n",
        "                             model='vamossyd/emtract-distilbert-base-uncased-emotion',\n",
        "                             device=device,\n",
        "                             model_kwargs={'torch_dtype': torch.float16} if device == 0 else {})\n",
        "\n",
        "def batch_classify_emotions(tweets, classifier, batch_size=128):\n",
        "    emotions = []\n",
        "    tweets_list = tweets.tolist()\n",
        "\n",
        "    # Filter valid tweets\n",
        "    valid_tweets = []\n",
        "    valid_indices = []\n",
        "    for idx, tweet in enumerate(tweets_list):\n",
        "        if pd.notna(tweet) and isinstance(tweet, str):\n",
        "            valid_tweets.append(tweet)\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    # Initialize results array with default values\n",
        "    results = ['UNKNOWN'] * len(tweets_list)\n",
        "\n",
        "    # Process in batches\n",
        "    print(f\"\\nProcessing {len(valid_tweets)} tweets in batches of {batch_size}...\")\n",
        "    for i in tqdm(range(0, len(valid_tweets), batch_size), desc=\"Classifying emotions\"):\n",
        "        batch = valid_tweets[i:i+batch_size]\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                batch_results = classifier(batch, top_k=None)\n",
        "\n",
        "            for result_idx, result in enumerate(batch_results):\n",
        "                # Extract top emotion label\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    top_emotion = result[0]['label']\n",
        "                elif isinstance(result, dict) and 'label' in result:\n",
        "                    top_emotion = result['label']\n",
        "                else:\n",
        "                    top_emotion = 'ERROR'\n",
        "\n",
        "                results[valid_indices[i + result_idx]] = top_emotion\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error in batch {i}: {str(e)}\")\n",
        "            for result_idx in range(len(batch)):\n",
        "                results[valid_indices[i + result_idx]] = 'ERROR'\n",
        "\n",
        "        # GPU memory cleanup\n",
        "        if device == 0 and (i // batch_size) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Apply emotion classification to dataset tweets\n",
        "print(f\"\\nDataset shape: {df0_model.shape}\")\n",
        "print(f\"Tweet column: 'Tweet'\")\n",
        "\n",
        "start_time = time.time()\n",
        "df0_model['emotion'] = batch_classify_emotions(df0_model['Tweet'], emotion_classifier, batch_size=128)\n",
        "classification_time = time.time() - start_time\n",
        "\n",
        "# Calculate statistics\n",
        "emotion_counts = df0_model['emotion'].value_counts()\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CLASSIFICATION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n Classification completed in {classification_time:.2f} seconds\")\n",
        "print(f\"  Processing speed: {len(df0_model)/classification_time:.2f} tweets/second\")\n",
        "print(f\"\\nEmotion Distribution:\")\n",
        "print(emotion_counts)\n",
        "\n",
        "# Save the new dataset with emotions\n",
        "df0_model.to_parquet('stock_tweets_with_emotions.parquet', index=False)\n",
        "print(f\"\\n Dataset saved: stock_tweets_with_emotions.parquet\")\n",
        "print(f\"  Rows: {len(df0_model)}, Columns: {len(df0_model.columns)}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Final GPU cleanup\n",
        "if device == 0:\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfR-bGY5ncKJ"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset with emotions loaded by emtract model\n",
        "df0_final = pd.read_parquet(\"/content/stock_tweets_with_emotions.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYiYNZCInuGu",
        "outputId": "0ba3863b-18cc-442b-bf82-f2d1fb016aa4"
      },
      "outputs": [],
      "source": [
        "# Print the new columns\n",
        "df0_final.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "Tj24FjRvnyZZ",
        "outputId": "8bed9d45-61a2-4089-8d67-f8aa4bf3d796"
      },
      "outputs": [],
      "source": [
        "# Print the structure of new dataset\n",
        "df0_final.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Gq5y8Wu_nFH0",
        "outputId": "8ab39732-bb06-49b7-e036-b7fb087524ec"
      },
      "outputs": [],
      "source": [
        "# Emotion distribution\n",
        "sns.set(style=\"white\")\n",
        "plt.figure(figsize=(8,4))\n",
        "df0_final['emotion'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Emotion distribution\")\n",
        "plt.ylabel(\"Tweets\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "KeOJMP4EnT0b",
        "outputId": "b77d3a90-f203-4031-e798-e2b30a97e686"
      },
      "outputs": [],
      "source": [
        "top_n = 5\n",
        "top_tickers = df0_final['Stock Name'].value_counts().head(top_n).index\n",
        "\n",
        "emo_ticker = pd.crosstab(df0_final['Stock Name'], df0_final['emotion'])\n",
        "emo_ticker = emo_ticker.loc[top_tickers]\n",
        "\n",
        "emo_ticker_pct = emo_ticker.div(emo_ticker.sum(axis=1), axis=0)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "emo_ticker.plot(kind='bar', stacked=True, ax=plt.gca())\n",
        "plt.title(\"Emotions per ticker (top tickers)\")\n",
        "plt.ylabel(\"Tweets\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ec4ukv9lRWp",
        "outputId": "5a8cc37d-7891-4336-d238-74a4aad7799d"
      },
      "outputs": [],
      "source": [
        "df0_final.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "JJvuN5wZoOqC",
        "outputId": "7984b869-5331-475f-cd63-8a6b8ee7b72c"
      },
      "outputs": [],
      "source": [
        "# Merge the datasets on date parameter\n",
        "merged_df = pd.merge(\n",
        "    df0_final,\n",
        "    df,\n",
        "    on=['Date','Stock Name'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "merged_df.drop(columns=['has_space','tweet_len'], inplace=True)\n",
        "# Display top 5 in the merged dataset\n",
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8TeVvfhSebY",
        "outputId": "4e3da144-3703-40ee-fd15-5e7b2c57e612"
      },
      "outputs": [],
      "source": [
        "# Display the columns in merged dataset\n",
        "merged_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8tr7EHYZmVH",
        "outputId": "58747386-c3c3-4ab8-c66b-912967ea4086"
      },
      "outputs": [],
      "source": [
        "# Validate the rows in the dataset\n",
        "merged_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3WZQPuDZrZo",
        "outputId": "a966e2c4-c967-4602-f97c-b27f7fc9b587"
      },
      "outputs": [],
      "source": [
        "# Validate whether there is null values in the merged dataset\n",
        "print(merged_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilnWTqmRlRWp"
      },
      "outputs": [],
      "source": [
        "merged_df.to_parquet('merged_stock_tweets_yfinance.parquet', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuRGl5Z1XLZJ",
        "outputId": "edaadbec-7edc-48ca-a8f3-23bca07d7415"
      },
      "outputs": [],
      "source": [
        "tickers = [\"TSLA\", \"TSM\", \"AAPL\", \"AMZN\", \"MSFT\", \"PG\", \"NIO\", \"META\", \"AMD\",\n",
        "           \"NFLX\", \"GOOG\", \"PYPL\", \"DIS\", \"BA\", \"COST\", \"INTC\", \"KO\", \"CRM\",\n",
        "           \"XPEV\", \"ENPH\", \"ZS\", \"VZ\", \"BX\", \"F\", \"NOC\"]\n",
        "\n",
        "all_ticker_data = []\n",
        "today = date.today().strftime('%Y-%m-%d')\n",
        "for ticker in tickers:\n",
        "    print(f\"Downloading data for {ticker}...\")\n",
        "    data = yf.download(ticker, start=\"2022-10-01\", end=today, \n",
        "                       auto_adjust=False, progress=False)\n",
        "    \n",
        "    if not data.empty:\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            data.columns = data.columns.get_level_values(0)\n",
        "        \n",
        "        data = data.reset_index()\n",
        "        data['Stock Name'] = ticker\n",
        "        \n",
        "        print(f\"  Columns: {data.columns.tolist()}\")\n",
        "        all_ticker_data.append(data)\n",
        "    else:\n",
        "        print(f\"No data found for {ticker}\")\n",
        "\n",
        "df_flat = pd.concat(all_ticker_data, ignore_index=True)\n",
        "\n",
        "df_flat = df_flat[['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Stock Name']]\n",
        "\n",
        "#  Export to CSV\n",
        "df_flat.to_csv(\"ohlcv_data.csv\", index=False)\n",
        "print(\"\\n SUCCESS: File saved as ohlcv_data.csv\")\n",
        "print(f\"Shape: {df_flat.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_flat.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBoaFtvP651F",
        "outputId": "eaefa2b8-d0b9-4de4-e591-8283a48fffec"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "CSV_FILE = \"cleaned_stock_news_2022_2026.csv\"\n",
        "\n",
        "# Date range\n",
        "START_DATE = datetime(2022, 10, 1)\n",
        "END_DATE = datetime.now()\n",
        "\n",
        "tickers = [\"TSLA\", \"TSM\", \"AAPL\", \"AMZN\", \"MSFT\", \"NVDA\", \"PLTR\", \"META\", \"AMD\",\n",
        "           \"NFLX\", \"GOOG\", \"PYPL\", \"DIS\", \"BA\", \"COST\", \"INTC\", \"KO\", \"CRM\",\n",
        "           \"XPEV\", \"ENPH\", \"ZS\", \"VZ\", \"BX\", \"F\", \"NOC\"]\n",
        "\n",
        "# Map tickers to full company names\n",
        "TICKER_NAMES = {\n",
        "    \"TSLA\": \"Tesla Inc\", \"TSM\": \"Taiwan Semiconductor Manufacturing Company\",\n",
        "    \"AAPL\": \"Apple Inc\", \"AMZN\": \"Amazon.com Inc\", \"MSFT\": \"Microsoft Corporation\",\n",
        "    \"NVDA\": \"NVIDIA Corporation\", \"PLTR\": \"Palantir Technologies Inc\",\n",
        "    \"META\": \"Meta Platforms Inc\", \"AMD\": \"Advanced Micro Devices Inc\",\n",
        "    \"NFLX\": \"Netflix Inc\", \"GOOG\": \"Alphabet Inc\", \"PYPL\": \"PayPal Holdings Inc\",\n",
        "    \"DIS\": \"The Walt Disney Company\", \"BA\": \"The Boeing Company\",\n",
        "    \"COST\": \"Costco Wholesale Corporation\", \"INTC\": \"Intel Corporation\",\n",
        "    \"KO\": \"The Coca-Cola Company\", \"CRM\": \"Salesforce Inc\",\n",
        "    \"XPEV\": \"XPeng Inc\", \"ENPH\": \"Enphase Energy Inc\", \"ZS\": \"Zscaler Inc\",\n",
        "    \"VZ\": \"Verizon Communications Inc\", \"BX\": \"Blackstone Inc\",\n",
        "    \"F\": \"Ford Motor Company\", \"NOC\": \"Northrop Grumman Corporation\"\n",
        "}\n",
        "\n",
        "# Junk keywords to filter out automated/low-quality content\n",
        "JUNK_KEYWORDS = [\n",
        "    \"overbought\", \"oversold\", \"form 4\", \"insider buy\", \"insider sell\",\n",
        "    \"filing\", \"sec filing\", \"nasdaq:\", \"nyse:\", \"earnings call transcript\",\n",
        "    \"stock alert\", \"stock pick\", \"buy now\", \"top stocks\", \"click here\",\n",
        "    \"subscribe\", \"sign up\", \"newsletter\", \"advertisement\", \"sponsored\",\n",
        "    \"price target\", \"technical analysis\", \"moved up\", \"moved down\",\n",
        "    \"unusual options\", \"options activity\", \"short interest\", \"stock screener\",\n",
        "    \"benzinga\", \"marketbeat\", \"tipranks\", \"stock rover\"\n",
        "]\n",
        "\n",
        "# Minimum word count for meaningful content\n",
        "MIN_WORD_COUNT = 15\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def scrub_text(text):\n",
        "    \"\"\"Clean the text of URLs, HTML tags, and messy whitespace.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"&\\w+;\", \"\", text)\n",
        "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "def is_quality_content(text):\n",
        "    \"\"\"Filter out junk content and short tweets.\"\"\"\n",
        "    if not text:\n",
        "        return False\n",
        "\n",
        "    # Check minimum word count\n",
        "    word_count = len(text.split())\n",
        "    if word_count < MIN_WORD_COUNT:\n",
        "        return False\n",
        "\n",
        "    # Check for junk keywords\n",
        "    text_lower = text.lower()\n",
        "    if any(keyword in text_lower for keyword in JUNK_KEYWORDS):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def split_date_range(start_date, end_date, months_per_chunk=3):\n",
        "    \"\"\"Split date range into smaller chunks to bypass 100-article limit.\"\"\"\n",
        "    chunks = []\n",
        "    current = start_date\n",
        "\n",
        "    while current < end_date:\n",
        "        chunk_end = min(current + timedelta(days=months_per_chunk * 30), end_date)\n",
        "        chunks.append((current, chunk_end))\n",
        "        current = chunk_end + timedelta(days=1)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def fetch_news_gnews(ticker, company_name, start_date, end_date):\n",
        "    \"\"\"Fetch news using GNews with chunked date ranges to get more articles.\"\"\"\n",
        "    all_news = []\n",
        "\n",
        "    # Split into 3-month chunks to get more than 100 articles\n",
        "    date_chunks = split_date_range(start_date, end_date, months_per_chunk=3)\n",
        "\n",
        "    for chunk_start, chunk_end in date_chunks:\n",
        "        try:\n",
        "            google_news = GNews(\n",
        "                language='en',\n",
        "                country='US',\n",
        "                start_date=(chunk_start.year, chunk_start.month, chunk_start.day),\n",
        "                end_date=(chunk_end.year, chunk_end.month, chunk_end.day),\n",
        "                max_results=100  # Max per chunk\n",
        "            )\n",
        "\n",
        "            # Search using both ticker and company name\n",
        "            search_query = f\"{ticker} OR {company_name} stock\"\n",
        "            news = google_news.get_news(search_query)\n",
        "\n",
        "            if news:\n",
        "                all_news.extend(news)\n",
        "                print(f\"    -> Found {len(news)} articles for {chunk_start.date()} to {chunk_end.date()}\")\n",
        "\n",
        "            time.sleep(0.5)  # Small delay between chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    -> Error for chunk {chunk_start.date()}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_news\n",
        "\n",
        "# --- Main Logic ---\n",
        "\n",
        "def main():\n",
        "    final_data = []\n",
        "\n",
        "    print(\"Starting Google News scraper with chunked date ranges...\")\n",
        "    print(f\"Date range: {START_DATE.date()} to {END_DATE.date()}\")\n",
        "    print(\"Splitting into 3-month chunks to get MORE than 100 articles per ticker\")\n",
        "    print(f\"Filters: Minimum {MIN_WORD_COUNT} words, removing junk keywords\\n\")\n",
        "    print(\"Note: Make sure 'gnews' is installed: pip install gnews\\n\")\n",
        "\n",
        "    for ticker in tickers:\n",
        "        company_name = TICKER_NAMES.get(ticker, ticker)\n",
        "        print(f\"\\nProcessing {ticker} ({company_name})...\")\n",
        "\n",
        "        news_items = fetch_news_gnews(ticker, company_name, START_DATE, END_DATE)\n",
        "        count = 0\n",
        "\n",
        "        for item in news_items:\n",
        "            # Get published date\n",
        "            pub_date = item.get(\"published date\", \"\")\n",
        "\n",
        "            # Get title and description\n",
        "            title = item.get(\"title\", \"\")\n",
        "            description = item.get(\"description\", \"\")\n",
        "\n",
        "            # Combine title and description\n",
        "            text = f\"{title}. {description}\" if description else title\n",
        "\n",
        "            cleaned_text = scrub_text(text)\n",
        "\n",
        "            # Apply quality filters\n",
        "            if is_quality_content(cleaned_text):\n",
        "                final_data.append({\n",
        "                    \"date\": pub_date,\n",
        "                    \"tweet\": cleaned_text,\n",
        "                    \"stock name\": ticker,\n",
        "                    \"company name\": company_name\n",
        "                })\n",
        "                count += 1\n",
        "\n",
        "        print(f\"  -> Total saved: {count} records\")\n",
        "\n",
        "    # Export to CSV\n",
        "    if final_data:\n",
        "        df = pd.DataFrame(final_data)\n",
        "        df = df[[\"date\", \"tweet\", \"stock name\", \"company name\"]]\n",
        "\n",
        "        # Remove duplicates based on tweet content\n",
        "        df = df.drop_duplicates(subset=[\"tweet\"])\n",
        "\n",
        "        # Sort by date\n",
        "        df = df.sort_values(\"date\")\n",
        "\n",
        "        df.to_csv(CSV_FILE, index=False)\n",
        "        print(f\"\\nSUCCESS: {len(df)} unique records saved to {CSV_FILE}\")\n",
        "        print(f\"\\nBreakdown by ticker:\")\n",
        "        ticker_counts = df[\"stock name\"].value_counts().sort_index()\n",
        "        for ticker, count in ticker_counts.items():\n",
        "            print(f\"  {ticker}: {count} articles\")\n",
        "        print(f\"\\nTotal unique articles: {len(df)}\")\n",
        "    else:\n",
        "        print(\"\\n No data found. Make sure 'gnews' is installed: pip install gnews\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test_financial_news_with_emotions = pd.read_csv(\"/content/cleaned_stock_news_2022_2026.csv\")\n",
        "# Loading the emtract model for consumption in data pipeline\n",
        "print(\"=\" * 70)\n",
        "print(\"EMOTION CLASSIFICATION - SIMPLIFIED PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "if device == 0:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\" GPU DETECTED: {gpu_name}\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\" Using CPU\")\n",
        "\n",
        "emotion_classifier = pipeline('text-classification',\n",
        "                             model='vamossyd/emtract-distilbert-base-uncased-emotion',\n",
        "                             device=device,\n",
        "                             model_kwargs={'torch_dtype': torch.float16} if device == 0 else {})\n",
        "\n",
        "def batch_classify_emotions(tweets, classifier, batch_size=128):\n",
        "    emotions = []\n",
        "    tweets_list = tweets.tolist()\n",
        "\n",
        "    # Filter valid tweets\n",
        "    valid_tweets = []\n",
        "    valid_indices = []\n",
        "    for idx, tweet in enumerate(tweets_list):\n",
        "        if pd.notna(tweet) and isinstance(tweet, str):\n",
        "            valid_tweets.append(tweet)\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    # Initialize results array with default values\n",
        "    results = ['UNKNOWN'] * len(tweets_list)\n",
        "\n",
        "    # Process in batches\n",
        "    print(f\"\\nProcessing {len(valid_tweets)} tweets in batches of {batch_size}...\")\n",
        "    for i in tqdm(range(0, len(valid_tweets), batch_size), desc=\"Classifying emotions\"):\n",
        "        batch = valid_tweets[i:i+batch_size]\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                batch_results = classifier(batch, top_k=None)\n",
        "\n",
        "            for result_idx, result in enumerate(batch_results):\n",
        "                # Extract top emotion label\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    top_emotion = result[0]['label']\n",
        "                elif isinstance(result, dict) and 'label' in result:\n",
        "                    top_emotion = result['label']\n",
        "                else:\n",
        "                    top_emotion = 'ERROR'\n",
        "\n",
        "                results[valid_indices[i + result_idx]] = top_emotion\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error in batch {i}: {str(e)}\")\n",
        "            for result_idx in range(len(batch)):\n",
        "                results[valid_indices[i + result_idx]] = 'ERROR'\n",
        "\n",
        "        # GPU memory cleanup\n",
        "        if device == 0 and (i // batch_size) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Apply emotion classification to dataset tweets\n",
        "print(f\"\\nDataset shape: {df_test_financial_news_with_emotions.shape}\")\n",
        "print(f\"Tweet column: 'tweet'\")\n",
        "\n",
        "start_time = time.time()\n",
        "df_test_financial_news_with_emotions['emotion'] = batch_classify_emotions(df_test_financial_news_with_emotions['tweet'], emotion_classifier, batch_size=128)\n",
        "classification_time = time.time() - start_time\n",
        "\n",
        "# Calculate statistics\n",
        "emotion_counts = df_test_financial_news_with_emotions['emotion'].value_counts()\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CLASSIFICATION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n Classification completed in {classification_time:.2f} seconds\")\n",
        "print(f\"  Processing speed: {len(df_test_financial_news_with_emotions)/classification_time:.2f} tweets/second\")\n",
        "print(f\"\\nEmotion Distribution:\")\n",
        "print(emotion_counts)\n",
        "\n",
        "# Save the new dataset with emotions\n",
        "df_test_financial_news_with_emotions.to_parquet('stock_tweets_with_emotions_test_data.parquet', index=False)\n",
        "print(f\"\\n Dataset saved: stock_tweets_with_emotions_test_data.parquet\")\n",
        "print(f\"  Rows: {len(df_test_financial_news_with_emotions)}, Columns: {len(df_test_financial_news_with_emotions.columns)}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Final GPU cleanup\n",
        "if device == 0:\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test_financial_news_with_emotions_cleaned = pd.read_parquet(\"/content/stock_tweets_with_emotions_test_data.parquet\")\n",
        "print(df_test_financial_news_with_emotions_cleaned.shape)\n",
        "\n",
        "# Use a lambda to format only valid dates and leave others untouched\n",
        "def safe_format(x):\n",
        "    try:\n",
        "        # Convert to datetime and then to your specific string format\n",
        "        return pd.to_datetime(x).strftime('%d/%m/%Y')\n",
        "    except:\n",
        "        return x\n",
        "\n",
        "df_test_financial_news_with_emotions_cleaned['date'] = df_test_financial_news_with_emotions_cleaned['date'].apply(safe_format)\n",
        "df_test_financial_news_with_emotions_cleaned.columns = df_test_financial_news_with_emotions_cleaned.columns.str.title()\n",
        "df_test_financial_news_with_emotions_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ohclv_test = pd.read_csv(\"/content/ohlcv_data.csv\")\n",
        "df_ohclv_test['Date'] = pd.to_datetime(df_ohclv_test['Date']).dt.strftime('%d/%m/%Y')\n",
        "df_ohclv_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge the datasets on date parameter\n",
        "\n",
        "merged_test_df = pd.merge(\n",
        "    df_test_financial_news_with_emotions_cleaned,\n",
        "    df_ohclv_test,\n",
        "    on=['Date','Stock Name'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Display top 5 in the merged dataset\n",
        "merged_test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_test_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_test_df.isnull().sum()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
